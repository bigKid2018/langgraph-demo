import os
from typing import Optional, Dict, Any
from langchain_openai import AzureChatOpenAI
from langchain_core.messages import AIMessage
from utils.config import Config


class LLMManager:
    """LLMç®¡ç†å™¨ - ç»Ÿä¸€çš„ LLM åˆå§‹åŒ–å’Œé…ç½®ç®¡ç†"""

    _instance = None
    _llm = None

    def __new__(cls):
        if cls._instance is None:
            cls._instance = super().__new__(cls)
        return cls._instance

    def __init__(self):
        if self._llm is None:
            self._initialize_llm()

    def _initialize_llm(self):
        """åˆå§‹åŒ– LLM å®ä¾‹"""
        # æ‰“å°é…ç½®çŠ¶æ€
        Config.print_config_status()

        # æ£€æŸ¥APIå¯†é’¥
        if not Config.is_api_key_set():
            print("âŒ æœªæ‰¾åˆ° AZURE_OPENAI_API_KEY ç¯å¢ƒå˜é‡")
            print("ğŸ”„ åˆ‡æ¢åˆ°æ¨¡æ‹Ÿæ¨¡å¼...")
            self._llm = MockLLM()
            self._llm_type = "mock"
            return

        try:
            # è·å– Azure OpenAI é…ç½®
            llm_config = Config.get_azure_openai_config()

            print(f"ğŸš€ æ­£åœ¨åˆå§‹åŒ–Azure OpenAI LLM: {llm_config['azure_deployment']}")
            self._llm = AzureChatOpenAI(**llm_config)
            self._llm_type = "azure_openai"
            print(f"âœ… Azure OpenAI LLMå·²æˆåŠŸåˆå§‹åŒ–: {llm_config['azure_deployment']}")
        except Exception as e:
            print(f"âš ï¸ æ— æ³•åˆå§‹åŒ–Azure OpenAI LLM: {e}")
            print("ğŸ”„ åˆ‡æ¢åˆ°æ¨¡æ‹Ÿæ¨¡å¼...")
            self._llm = MockLLM()
            self._llm_type = "mock"

    def get_llm(self):
        """è·å– LLM å®ä¾‹"""
        return self._llm

    def get_llm_type(self):
        """è·å–å½“å‰ä½¿ç”¨çš„ LLM ç±»å‹"""
        return self._llm_type

    def is_mock(self):
        """åˆ¤æ–­æ˜¯å¦ä½¿ç”¨æ¨¡æ‹Ÿæ¨¡å¼"""
        return self._llm_type == "mock"


class MockLLM:
    """æ¨¡æ‹Ÿ LLM - ç”¨äºæµ‹è¯•å’Œå¼€å‘"""

    def __init__(self):
        self.model = "mock-gpt"
        self.temperature = 0

    def invoke(self, messages):
        """æ¨¡æ‹Ÿ LLM è°ƒç”¨"""
        # è·å–æœ€åä¸€æ¡æ¶ˆæ¯
        if not messages:
            return AIMessage(content="Hello! How can I help you?")

        last_message = messages[-1]
        if isinstance(last_message, tuple):
            user_content = last_message[1]
        elif hasattr(last_message, 'content'):
            user_content = last_message.content
        else:
            user_content = str(last_message)

        # ç®€å•çš„å›å¤é€»è¾‘
        user_content_lower = user_content.lower()

        if "hello" in user_content_lower or "hi" in user_content_lower:
            return AIMessage(content="Hello! How can I help you today?")
        elif "bye" in user_content_lower or "goodbye" in user_content_lower:
            return AIMessage(content="Goodbye! Have a great day!")
        elif "how are you" in user_content_lower:
            return AIMessage(content="I'm doing well, thank you! How can I assist you?")
        elif "joke" in user_content_lower:
            return AIMessage(content="Why don't scientists trust atoms? Because they make up everything! ğŸ˜„")
        elif "weather" in user_content_lower:
            return AIMessage(content="I'm sorry, I don't have access to current weather information.")
        else:
            return AIMessage(content=f"I understand you said: '{user_content}'. How can I help you with that?")


# å…¨å±€LLMç®¡ç†å™¨å®ä¾‹
llm_manager = LLMManager()


# ä¾¿æ·å‡½æ•°
def get_llm():
    """è·å–LLMå®ä¾‹çš„ä¾¿æ·å‡½æ•°"""
    return llm_manager.get_llm()


def get_llm_info():
    """è·å–LLMä¿¡æ¯"""
    return {
        "type": llm_manager.get_llm_type(),
        "is_mock": llm_manager.is_mock(),
        "azure_deployment": Config.AZURE_OPENAI_CHAT_DEPLOYMENT_NAME,
        "azure_endpoint": Config.AZURE_OPENAI_ENDPOINT,
        "api_version": Config.AZURE_OPENAI_API_VERSION,
        "temperature": Config.LLM_TEMPERATURE,
        "max_tokens": Config.LLM_MAX_TOKENS
    }


# é…ç½®å‡½æ•°
def configure_llm(azure_deployment: Optional[str] = None,
                  temperature: Optional[float] = None,
                  max_tokens: Optional[int] = None,
                  azure_endpoint: Optional[str] = None,
                  api_version: Optional[str] = None):
    """åŠ¨æ€é…ç½®Azure OpenAI LLMå‚æ•°"""
    if azure_deployment:
        os.environ["AZURE_OPENAI_CHAT_DEPLOYMENT_NAME"] = azure_deployment
        Config.AZURE_OPENAI_CHAT_DEPLOYMENT_NAME = azure_deployment
    if temperature is not None:
        os.environ["LLM_TEMPERATURE"] = str(temperature)
        Config.LLM_TEMPERATURE = temperature
    if max_tokens is not None:
        os.environ["LLM_MAX_TOKENS"] = str(max_tokens)
        Config.LLM_MAX_TOKENS = max_tokens
    if azure_endpoint is not None:
        os.environ["AZURE_OPENAI_ENDPOINT"] = azure_endpoint
        Config.AZURE_OPENAI_ENDPOINT = azure_endpoint
    if api_version is not None:
        os.environ["AZURE_OPENAI_API_VERSION"] = api_version
        Config.AZURE_OPENAI_API_VERSION = api_version

    # é‡æ–°åˆå§‹åŒ–LLM
    llm_manager._llm = None
    llm_manager._initialize_llm()

    return llm_manager.get_llm()
